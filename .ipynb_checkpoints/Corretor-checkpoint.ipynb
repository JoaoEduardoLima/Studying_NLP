{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07994c13",
   "metadata": {},
   "source": [
    "# Corretor Ortográfico\n",
    "\n",
    "Sempre quando queremos aprender novos conhecimentos, buscamos mais informações sobre o assunto em livros, vídeos, cursos, palestras, aulas e etc. Para que nosso modelo corrija as palavras, precisaremos primeiro ensiná-lo a escrever através de um vocabulário que cresce conforme aprende.\n",
    "\n",
    "Logo, iremos precisar de uma base de dados.\n",
    "\n",
    "Porém, como estamos trabalhando com o buscador do site da Alura, seria interessante também que essa base de conhecimento tivesse termos mais técnicos da área.\n",
    "\n",
    "A base de dados que utilizaremos neste curso será construída com os próprios artigos do Blog da Alura, pois ensinaremos o nosso corretor a realizar correções específicas para o mundo técnico de desenvolvimento, como Java, programação orientada a objeto, Data Science e etc.\n",
    "\n",
    "Quando trabalhamos em NLP, a nossa base de dados é conhecida como corpus. Ou seja, é um corpo composto por com diversos textos, e cada um corresponde a um artigo do nosso blog, chamado de documento.\n",
    "\n",
    "Logo, o corpus é um conjunto de documentos em Processamento Linguagem Natural, e no nosso caso, é composto pelos artigos do blog que formam a base de dados artigos.txt. Em seguida, a leremos no notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fa3e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "imagem \n",
      "\n",
      "Temos a seguinte classe que representa um usuário no nosso sistema:\n",
      "\n",
      "java\n",
      "\n",
      "Para salvar um novo usuário, várias validações são feitas, como por exemplo: Ver se o nome só contém letras, [**o CPF só números**] e ver se o usuário possui no mínimo 18 anos. Veja o método que faz essa validação:\n",
      "\n",
      "java \n",
      "\n",
      "Suponha agora que eu tenha outra classe, a classe `Produto`, que contém um atributo nome e eu quero fazer a mesma validação que fiz para o nome do usuário: Ver se só contém letras. E aí? Vou\n"
     ]
    }
   ],
   "source": [
    "with open(\"corpus/artigos.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    artigos = f.read()\n",
    "\n",
    "print(artigos[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fa255",
   "metadata": {},
   "source": [
    "Imaginemos que dois estudantes estão aprendendo um novo idioma, o russo.\n",
    "\n",
    "Ao final de um ano de estudos, o primeiro leu apenas um livro de 100 páginas, enquanto o outro leu toda a coleção do Game of Thrones em russo, sendo que cada parte possui quase mil páginas. Neste caso, a segunda pessoa possui muito mais informações sobre a língua, pois teve acesso à um vocabulário muito maior, inclusive estando apta a fazer correções.\n",
    "\n",
    "Portanto, a quantidade de vocábulos é interessante para este aprendizado, o que também vale para o nosso corretor.\n",
    "\n",
    "Para sabermos se o nosso corpus é ideal para o trabalho, deveremos saber quantas palavras possui. Afinal, um texto com dez palavras faz no máximo dez correções por exemplo. Então precisaremos ter um corpus com um grande volume de termos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1295cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2605046"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(artigos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa665d0",
   "metadata": {},
   "source": [
    "Neste caso, utilizar apenas a função len() recebendo artigos não nos ajudará, pois nos retornará apenas a quantidade de caracteres presentes no corpus, e não a de palavras que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71b173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_exemplo = \"Olá, tudo bem?\"\n",
    "tokens = texto_exemplo.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d0cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24cb3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Olá,', 'tudo', 'bem?']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f4f6d",
   "metadata": {},
   "source": [
    "### Separando Palavras de Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf5a98",
   "metadata": {},
   "source": [
    "Porém, nem a vírgula da primeira parte em 'Olá,' e nem o ponto de interrogação na terceira em 'bem?' fazem parte dessas palavras em si.\n",
    "\n",
    "De qualquer forma o método split() pode nos ajudar neste caso. Se imprimirmos o len() de tokens, teremos o tamanho do vetor com os três itens retornados no comando anterior.\n",
    "\n",
    "Mas queremos os vocábulos separados, e não concatenados com pontuação.\n",
    "\n",
    "Em Processamento de Linguagem Natural, cada um dos elementos separados desse vetor é conhecido como token. Ou seja, ao pegarmos as strings e as separarmos em pequenos pedaços, estamos tokenizando a frase.\n",
    "\n",
    "Portanto, ainda temos apenas tokens separados, e não palavras separadas propriamente ditas.\n",
    "\n",
    "Uma ferramenta bem conhecida na área de NLP é o nltk ou Natural Language Tool Kit, que é um conjunto de ferramentas que implementa diversos métodos e algoritmos para análise textual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8420774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\joaoe\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joaoe\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\joaoe\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\joaoe\\anaconda3\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\joaoe\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\joaoe\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b99a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d87e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joaoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "palavras_separadas = nltk.tokenize.word_tokenize(texto_exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25264551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Olá', ',', 'tudo', 'bem', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_separadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125ff7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(palavras_separadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea600ec7",
   "metadata": {},
   "source": [
    "Ou seja, a frase \"Olá, tudo bem?\" possui cinco tokens, sendo três palavras e duas pontuações, mas precisamos descobrir apenas a quantidade de vocábulos no corpus.\n",
    "\n",
    "Para fazermos a separação dos tokens, criaremos uma função \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8cb9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Olá', 'tudo', 'bem']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separa_palavras(lista_tokens):\n",
    "    lista_palavras = []\n",
    "    for token in lista_tokens:\n",
    "        if token.isalpha():             #isalpha() selecionará apenas as letras do alfabeto, retornando-as como verdadeiro\n",
    "            lista_palavras.append(token)\n",
    "    return lista_palavras\n",
    "\n",
    "separa_palavras(palavras_separadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4aeec",
   "metadata": {},
   "source": [
    "Então, vamos aplicar este método em nosso corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f228317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número de palavras é 403106\n"
     ]
    }
   ],
   "source": [
    "lista_tokens = nltk.tokenize.word_tokenize(artigos)\n",
    "lista_palavras = separa_palavras(lista_tokens)\n",
    "print(f\"O número de palavras é {len(lista_palavras)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb49ca",
   "metadata": {},
   "source": [
    "O valor total de palavras retornado não representa a quantidade de palavras únicas que podem ser corrigidas, afinal muitas delas aparecerão *repetidas *nos textos.\n",
    "\n",
    "Portanto, precisaremos calcular quantos vocábulos únicos existem sem repetição."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41644d",
   "metadata": {},
   "source": [
    "### Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05dcb4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imagem', 'temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no']\n"
     ]
    }
   ],
   "source": [
    "def normalizacao(lista_palavras):      # Esta função retorna uma lista com as letras todas em minuscúlas.\n",
    "    lista_normalizada = []\n",
    "    for palavra in lista_palavras:\n",
    "        lista_normalizada.append(palavra.lower())\n",
    "    return lista_normalizada\n",
    "\n",
    "lista_normalizada = normalizacao(lista_palavras)\n",
    "print(lista_normalizada[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e17ad1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18465"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(lista_normalizada))        # set() remove as repetições de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfc516",
   "metadata": {},
   "source": [
    "Este será o número de palavras que nosso corretor ortográfico aprenderá a corrigir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08368fa",
   "metadata": {},
   "source": [
    "## Desenvolvendo e testanto o corretor\n",
    "\n",
    "### Inserindo uma letra\n",
    "\n",
    "Usando como exemplo a palavra digitada errada \" Lgica \" onde o certo seria \" Lógica \".\n",
    "\n",
    "Nosso modelo de NLP precisará de um algoritmo capaz de inserir uma letra a mais neste termo digitado equivocadamente.\n",
    "\n",
    "\n",
    "\n",
    "| ESQUERDO | DIREITO |     OPERAÇÃO     | RESULTADO |\n",
    "|:--------:|:-------:|:----------------:|:---------:|\n",
    "| null     | lgica   | null + ó + lgica | óligca    |\n",
    "| l        | gica    | l + ó + gica     | lógica    |\n",
    "| lg       | ica     | lg + ó + ica     | lgóica    |\n",
    "| lgi      | ca      | lgi + ó + ca     | lgióca    |\n",
    "| lgic     | a       | lgic + ó + a     | lgicóa    |\n",
    "| lgica    | null    | lgica + ó + null | lgicaó    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce9e9a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algica', 'blgica', 'clgica', 'dlgica', 'elgica', 'flgica', 'glgica', 'hlgica', 'ilgica', 'jlgica', 'klgica', 'llgica', 'mlgica', 'nlgica', 'olgica', 'plgica', 'qlgica', 'rlgica', 'slgica', 'tlgica', 'ulgica', 'vlgica', 'wlgica', 'xlgica', 'ylgica', 'zlgica', 'àlgica', 'álgica', 'âlgica', 'ãlgica', 'èlgica', 'élgica', 'êlgica', 'ìlgica', 'ílgica', 'îlgica', 'òlgica', 'ólgica', 'ôlgica', 'õlgica', 'ùlgica', 'úlgica', 'ûlgica', 'çlgica', 'lagica', 'lbgica', 'lcgica', 'ldgica', 'legica', 'lfgica', 'lggica', 'lhgica', 'ligica', 'ljgica', 'lkgica', 'llgica', 'lmgica', 'lngica', 'logica', 'lpgica', 'lqgica', 'lrgica', 'lsgica', 'ltgica', 'lugica', 'lvgica', 'lwgica', 'lxgica', 'lygica', 'lzgica', 'làgica', 'lágica', 'lâgica', 'lãgica', 'lègica', 'légica', 'lêgica', 'lìgica', 'lígica', 'lîgica', 'lògica', 'lógica', 'lôgica', 'lõgica', 'lùgica', 'lúgica', 'lûgica', 'lçgica', 'lgaica', 'lgbica', 'lgcica', 'lgdica', 'lgeica', 'lgfica', 'lggica', 'lghica', 'lgiica', 'lgjica', 'lgkica', 'lglica', 'lgmica', 'lgnica', 'lgoica', 'lgpica', 'lgqica', 'lgrica', 'lgsica', 'lgtica', 'lguica', 'lgvica', 'lgwica', 'lgxica', 'lgyica', 'lgzica', 'lgàica', 'lgáica', 'lgâica', 'lgãica', 'lgèica', 'lgéica', 'lgêica', 'lgìica', 'lgíica', 'lgîica', 'lgòica', 'lgóica', 'lgôica', 'lgõica', 'lgùica', 'lgúica', 'lgûica', 'lgçica', 'lgiaca', 'lgibca', 'lgicca', 'lgidca', 'lgieca', 'lgifca', 'lgigca', 'lgihca', 'lgiica', 'lgijca', 'lgikca', 'lgilca', 'lgimca', 'lginca', 'lgioca', 'lgipca', 'lgiqca', 'lgirca', 'lgisca', 'lgitca', 'lgiuca', 'lgivca', 'lgiwca', 'lgixca', 'lgiyca', 'lgizca', 'lgiàca', 'lgiáca', 'lgiâca', 'lgiãca', 'lgièca', 'lgiéca', 'lgiêca', 'lgiìca', 'lgiíca', 'lgiîca', 'lgiòca', 'lgióca', 'lgiôca', 'lgiõca', 'lgiùca', 'lgiúca', 'lgiûca', 'lgiçca', 'lgicaa', 'lgicba', 'lgicca', 'lgicda', 'lgicea', 'lgicfa', 'lgicga', 'lgicha', 'lgicia', 'lgicja', 'lgicka', 'lgicla', 'lgicma', 'lgicna', 'lgicoa', 'lgicpa', 'lgicqa', 'lgicra', 'lgicsa', 'lgicta', 'lgicua', 'lgicva', 'lgicwa', 'lgicxa', 'lgicya', 'lgicza', 'lgicàa', 'lgicáa', 'lgicâa', 'lgicãa', 'lgicèa', 'lgicéa', 'lgicêa', 'lgicìa', 'lgicía', 'lgicîa', 'lgicòa', 'lgicóa', 'lgicôa', 'lgicõa', 'lgicùa', 'lgicúa', 'lgicûa', 'lgicça', 'lgicaa', 'lgicab', 'lgicac', 'lgicad', 'lgicae', 'lgicaf', 'lgicag', 'lgicah', 'lgicai', 'lgicaj', 'lgicak', 'lgical', 'lgicam', 'lgican', 'lgicao', 'lgicap', 'lgicaq', 'lgicar', 'lgicas', 'lgicat', 'lgicau', 'lgicav', 'lgicaw', 'lgicax', 'lgicay', 'lgicaz', 'lgicaà', 'lgicaá', 'lgicaâ', 'lgicaã', 'lgicaè', 'lgicaé', 'lgicaê', 'lgicaì', 'lgicaí', 'lgicaî', 'lgicaò', 'lgicaó', 'lgicaô', 'lgicaõ', 'lgicaù', 'lgicaú', 'lgicaû', 'lgicaç']\n"
     ]
    }
   ],
   "source": [
    "palavra_exemplo = \"lgica\"\n",
    "\n",
    "def insere_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
    "    for E, D in fatias:\n",
    "        for letra in letras:\n",
    "            novas_palavras.append(E + letra + D)\n",
    "    return novas_palavras\n",
    "\n",
    "def gerador_palavras(palavra):\n",
    "    fatias = []\n",
    "    for i in range(len(palavra)+1):\n",
    "        fatias.append((palavra[:i],palavra[i:]))\n",
    "    palavras_geradas = insere_letras(fatias)\n",
    "    return palavras_geradas\n",
    "\n",
    "palavras_geradas = gerador_palavras(palavra_exemplo)\n",
    "print(palavras_geradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3c738",
   "metadata": {},
   "source": [
    "Como saída de palavras_geradas, teremos todas as palavras geradas por inserção.\n",
    "\n",
    "Precisaremos de uma função que indicará 'lógica' como a palavra correta dentre as demais.\n",
    "\n",
    "Temos um arquivo de texto, e queremos verificar se um termo está correto ou não de acordo com sua existência em nossa base de dados. Também deveremos calcular suas probabilidades dentro do conjunto de palavras geradas, pegando a correta como a de máxima probabilidade.\n",
    "\n",
    "Pra construirmos a função probabilidade() em uma nova célula, primeiro precisaremos fazer este cálculo com uma palavra; a frequência do termo “lógica” será a quantidade de vezes que este parece no corpus em relação ao total de palavras. Para calcular, o nltk possui a função .FreqDist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "726d274f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 15502),\n",
       " ('o', 14056),\n",
       " ('que', 12230),\n",
       " ('a', 11099),\n",
       " ('e', 10501),\n",
       " ('para', 7710),\n",
       " ('um', 6368),\n",
       " ('é', 5899),\n",
       " ('uma', 5220),\n",
       " ('do', 5124)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia = nltk.FreqDist(lista_normalizada)\n",
    "frequencia.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c84d962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia[\"lógica\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af6c245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_palavras = len(lista_normalizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b25a87c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023815075935361914"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def probabilidade(palavra_gerada):\n",
    "    return frequencia[palavra_gerada]/total_palavras\n",
    "\n",
    "probabilidade(\"lógica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a76ac646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corretor(palavra):\n",
    "    palavras_geradas = gerador_palavras(palavra)\n",
    "    palavra_correta = max(palavras_geradas, key=probabilidade)\n",
    "    return palavra_correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88cc4999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lógica'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corretor(palavra_exemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200ce91",
   "metadata": {},
   "source": [
    "A saída do corretor será justamente a palavra 'lógica' mais frequente, a qual é a correção para quando digitamos por exemplo “lgica”. \n",
    "\n",
    "Porém, só estamos corrigindo o tipo de erro onde uma letra não foi digitada, e é interessante tratarmos outros casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e4c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
